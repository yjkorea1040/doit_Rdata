exam <- read.csv("csv_exam.csv")
View(exam)
exam[]
exam[1,]
exam[2,]
exam[exam$class==1,]
exam]exam$math>=80,]
exam[exam$math>=80,]
exam[,2:4]
exam[exam$class==1 & exam$math>=50,]
exam[exam$english<90 | exam$science<50,]
exam[,1]
#변수(속성)이름으로 - 선택(추출)
exam[, "class"]
exam[, "math"]
exam[, "science"]
exam[, "english"]
# C 사용하기
exma=c["class", "math" "english"]
# C 사용하기
exma=c["class", "math", "english"]
# C 사용하기
exma=[,c("class", "math", "english")]
# C 사용하기
exma=[,c("class", "math", "english")]
# C 사용하기
exam=[,c("class", "math", "english")]
# C 사용하기
exam[,c("class", "math", "english")]
exam[1,3]
exam[1,"math"]
exam[1,c("class","math","english")]
#1)내장함수
exam$tot<-(exam$math+exam$english+exam$science)/3
aggregate(data=exam[exam$math>=50&exam$english>=80,],tot~class, mean)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
#2)dplyr함수
exam %>%
filter(math>=50&english>=80) %>%
mutate(tot=(math+english+science)/3) %>%
grroup_by(class) %>%
summarise(mean=mean(tot))
library(dplyr)
exam %>%
filter(math>=50&english>=80) %>%
mutate(tot=(math+english+science)/3) %>%
grroup_by(class) %>%
summarise(mean=mean(tot))
exam %>%
filter(math>=50&english>=80) %>%
mutate(tot=(math+english+science)/3) %>%
grroup_by(class) %>%
summarise(mean=mean(tot))
exam %>%
filter(math>=50&english>=80) %>%
mutate(tot=(math+english+science)/3) %>%
group_by(class) %>%
summarise(mean=mean(tot))
#mpg 데이터 사용해서 분석계산하기
library(ggplot2)
mpg <- as.data.frame(ggplot::mpg)
#mpg 데이터 사용해서 분석계산하기
library(ggplot2)
mpg <- as.data.frame(ggplot::mpg)
mpg<-as.data.frame(ggplot::mpg)
#mpg 데이터 사용해서 분석계산하기
install.packages(ggplot2)
library(ggplot2)
mpg<-as.data.frame(ggplot::mpg)
library(dplyr)
mpg<-as.data.frame(ggplot::mpg)
library(ggplot2)
ggplot2
mpg %>%
mutate(tot=(cty+hwy)/2) %>%
filter(class=="compact"|class=="suv") %>%
group_by(class) %>%
summarise(mean_tot=mean(tot))
mpg<-as.data.frame(ggplot::mpg)
library(ggplot2)
#mpg 데이터 사용해서 분석계산하기
install.packages(ggplot)
install.packages(ggplot)
install.packages(ggplot2)
library(ggplot2)
mpg %>%
mutate(tot=(cty+hwy)/2) %>%
filter(class=="compact"|class=="suv") %>%
group_by(class) %>%
summarise(mean_tot=mean(tot))
var1<-c(1,2,3,1,2)
var2<-factor(c(1,2,3,1,2))
var1
var2
Class(var1) #numeric
Class(var1)
str(var1) # -> 숫자 연속형 변수로 계산 가능
str(var2) # -> 범주형 변수 -> 계산 불가
class(var1)
class(var2) #factor
levels(var1)
levels(var2)
var1+2
var2+2
var3<-c("a","b","c","b","e")
var4<-factor(c("a","b","c","b","e"))
var3
var4
var2<-as.numeric(var2)
mean(Var2)
mean(var2)
clas(var2)
class(var2)
levels(var2)
as.Date(var1)
as.numeric(var2)
a
a<-1
a
[1]1
b<-"hello"
b
class(a)
class(b)
a<-1
a
b<-"hello"
b
class(a)
class(b)
levels(a)
levels(b)
c<-c(1,2,3,4)
d<-c('a','b','c','d')
c
d
levels(c)
class(c)
levels(d)
class(d)
install.packages("rJava")
install.packages("memoise")
Sys.getenv("JAVA_HOME")
Sys.getenv("JAVA_HOME")
Sys.getenv("JAVA_HOME")
install.packages("rJava")
install.packages("memoise")
# install.packages("KoNLP")
install.packages("tm")
install.packages("multilinguer")
install.packages(c('stringr', 'hash', 'tau', 'Sejong', 'RSQLite', 'devtools'), type = "binary")
install.packages("multilinguer")
install.packages("https://mran.microsoft.com/snapshot/2017-12-11/bin/windows/contrib/3.4/KoNLP_0.80.1.zip", repos=NULL, type = "binary")
# 패키지 로드
library(rJava)
library(tm)
library(multilinguer)
library(KoNLP)
library(dplyr)
#2.사전설정하기
useNIADic()
useNIADic()
useNIADic()
useNIADic()
txt<-readLines("../datas/hiphop.txt")
txt<-readLines("C:/works/works_r/ch09/ch15/hiphop.txt")
txt<-readLines("C:/works/works_r/ch09/ch15/hiphop.txt")
head(txt)
View(txt)
is.jamo()
is.jaeum()
txt<-readLines("C:/works/works_r/ch09/ch15/hiphop.txt")
head(txt)
View(txt)
txt<-readLines("C:/works/works_r/ch09/ch15/hiphop.txt")
head(txt)
View(txt)
txt<-readLines("hiphop.txt")
head(txt)
library(rJava)
library(tm)
library(multilinguer)
library(KoNLP)
library(dplyr)
txt<-readLines("C:/works/works_r/ch09/ch15/hiphop.txt")
head(txt)
View(txt)
install.packages("readxl")
library(readxl)
songlist <-read_excel("../datas/SongList.xlsx")
Sys.getenv("JAVA_HOME")
install.packages("rJava")
install.packages("rJava")
install.packages("memoise")
# install.packages("KoNLP")
install.packages("tm")
install.packages(c('stringr', 'hash', 'tau', 'Sejong', 'RSQLite', 'devtools'), type = "binary")
install.packages("multilinguer")
install.packages("https://mran.microsoft.com/snapshot/2017-12-11/bin/windows/contrib/3.4/KoNLP_0.80.1.zip", repos=NULL, type = "binary")
library(rJava)
library(tm)
library(multilinguer)
library(KoNLP)
library(dplyr)
useNIADic()
txt<-readLines("C:/works/works_r/ch09/ch15/hiphop.txt")
head(txt)
View(txt)
songlist <-read_excel("../datas/SongList.xlsx")
install.packages("readxl")
library(readxl)
songlist <-read_excel("../datas/SongList.xlsx")
txt<-readLines("C:/works/works_r/ch09/ch15/hiphop.txt")
txt<-readLines("C:\works\works_r\ch09\ch15/hiphop.txt")
txt<-readLines("C:/works/works_r/ch09/ch15/hiphop.txt")
songlist <-read_excel("C:/works/works_r/ch09/ch15/SongList.xlsx")
head(songlist)
txt<-readLines("C:/works/works_r/ch09/ch15/hiphop.txt")
head(txt)
View(txt)
install.packages("stringr")
install.packages("stringr")
library(stringr)
head(txt,20)
txt2 <- str_replace_all(txt, "\\W", " ")
head(txt2)
# 1. 명사 추출하기
sentences1<-"대한민국의 영토는 한반도와 그 부속도서로 한다"
sentences1<-"대한민국의 영토는 한반도와 그 부속도서로 한다"
extractNoun(sentences = sentences1,  autoSpacing = T)
extractNoun(sentences1)
libraray(extractNoun)
library(extractNoun)
library(KoNLP)
library(extractNoun)
extractNoun(sentences = sentences1,  autoSpacing = T)
extractNoun(sentences1)
extractNoun(sentences = sentences1,  autoSpacing = F)
?unlist
j<-list(name="Joe", salary=55000, union=T)
View(j)
str(j)
names(j)
uj <-unlist(j)
View(uj)
str(uj)
class(uj)
levels(uj)
classs(j)
class(j)
l1 <- list(a = "a", b = 2, c = pi+2i)
l1
unlist(l1) # a character vector
l2 <- list(a = "a", b = as.name("b"), c = pi+2i)
l2
unlist(l2) # remains a list
# 가사에서 명사 추출
nouns<- extractNoun(txt)
nouns
View(nouns)
# 추출한 명사 list를 문자열 벡터로 변환, 단어별 빈도표 생성
ul_nouns<-unlist(nouns)
View(ul_nouns)
wordcount <-table(ul_nouns)
View(wordcount)
str(wordcount)
#데이터 프레임으로 변환
df_word<-as.data.frame(wordcount, stringsAsFactors = F)
View(df_word)
head(df_word)
#변수명 수정
df_word<-rename(df_word, word=ul_nouns, freq=Freq)
head(df_word)
library(rename)
library(dyplr)
library(dpylr)
library(dplyr)
#변수명 수정
df_word<-rename(df_word, word=ul_nouns, freq=Freq)
head(df_word)
# 4. 빈도 순으로 정렬 후 상위 20개 단어 추출
top_20<-df_word %>%
arrange(desc(freq)) %>%
head(20)
top_20
head(top_20)
df_word<-filter(df_word, nchar(word)>=2)
df_word
df_word<-filter(df_word, nchar(word)>=10)
df_word
# 1. 패키지 준비하기
install.packages("wordcloud")
library(wordcloud)
# 1. 패키지 준비하기
install.packages("wordcloud")
library(wordcloud)
library(RColorBrewer)
library(wordcloud)
library(RColorBrewer)
# 2. 단어 색상 목록 만들기
# Dark2 색상 목록에서 8개 색상 추출(hex color code)
pal<-brewer.pal(8, "Dark2")
pal
# 3. 난수 고정하기
# 매번 다른 모양의 워드 클라우드를 생성합니다.
set.seed(1234)
set.seed(1234)
# 4. 워드 클라우드 만들기
# wordclound()의 설정 내용과 뷰어창의 크기에 따라 보여지는 클라우드 모양 결정
wordcloud(words = df_word$word,  # 단어
freq = df_word$freq,   # 빈도
min.freq = 2,          # 최소 단어 빈도
max.words = 200,       # 표현 단어 수
random.order = F,      # 고빈도 단어 중앙 배치
rot.per = .1,          # 회전 단어 비율
scale = c(4, 0.3),     # 단어 크기 범위
colors = pal)          # 색깔 목록
#변수명 수정
df_word<-rename(df_word, word=ul_nouns, freq=Freq)
df_word<-as.data.frame(wordcount, stringsAsFactors = F)
View(df_word)
head(df_word)
#변수명 수정
df_word<-rename(df_word, word=ul_nouns, freq=Freq)
head(df_word)
# 4. 워드 클라우드 만들기
# wordclound()의 설정 내용과 뷰어창의 크기에 따라 보여지는 클라우드 모양 결정
wordcloud(words = df_word$word,  # 단어
freq = df_word$freq,   # 빈도
min.freq = 2,          # 최소 단어 빈도
max.words = 200,       # 표현 단어 수
random.order = F,      # 고빈도 단어 중앙 배치
rot.per = .1,          # 회전 단어 비율
scale = c(4, 0.3),     # 단어 크기 범위
colors = pal)          # 색깔 목록
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
max.words = 200,
random.order = F,
rot.per = .1,
scale = c(10, 0.3),
colors = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
max.words = 200,
random.order = F,
rot.per = .1,
scale = c(5, 0.3),
colors = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
max.words = 200,
random.order = F,
rot.per = .5,
scale = c(5, 0.3),
colors = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
max.words = 200,
random.order = F,
rot.per = 1.0,
scale = c(5, 0.3),
colors = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
max.words = 200,
random.order = F,
rot.per = 8.0,
scale = c(5, 0.3),
colors = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
max.words = 200,
random.order = F,
rot.per = 0.1,
scale = c(5, 0.3),
colors = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
max.words = 200,
random.order = T,
rot.per = 0.1,
scale = c(5, 0.3),
colors = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
max.words = 200,
random.order = F,
rot.per = 0.1,
scale = c(5, 0.3),
colors = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
max.words = 200,
random.order = F,
rot.per = .1,
scale = c(5, 0.3),
colors = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 10,
max.words = 200,
random.order = F,
rot.per = .1,
scale = c(5, 0.3),
colors = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 10,
max.words = 200,
random.order = F,
rot.per = .7,
scale = c(5, 0.3),
colors = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 10,
max.words = 200,
random.order = F,
rot.per = .1,
scale = c(4, 0.8),
colors = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
max.words = 200,
random.order = F,
rot.per = .1,
scale = c(4, 0.1),
colors = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
max.words = 200,
random.order = F,
rot.per = .1,
scale = c(4, 0.8),
colors = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 20,
max.words = 200,
random.order = F,
rot.per = .1,
scale = c(4, 0.8),
colors = pal)
# 많이 사용된 단어는 가운데 배치되고 글자 폰트도 크게 적용
top_20<-df_word %>%
arrange(desc(freq)) %>%
head(20)
top_20
head(top_20)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
max.words = 200,
random.order = F,
rot.per = .1,
scale = c(4, 0.3),
colors = pal)
# 5. 단어 색상 바꾸기
# 파란색 계열의 색상목록,
# 빈도가 높을수록 진한 파란색으로 표현
# Blues 색상에서 9개의 색상코드 추출
pal<-brewer.pal(9, "Blues")
pal
set.seed(1234)                     # 난수 고정
wordcloud(words = df_word$word,    # 단어
freq = df_word$freq,     # 빈도
min.freq = 2,            # 최소 단어 빈도
max.words = 200,         # 표현 단어 수
random.order = F,        # 고빈도 단어 중앙 배치
rot.per = .1,            # 회전 단어 비율
scale = c(4, 0.3),       # 단어 크기 범위
colors = pal)
twitter<- read.csv("C:/works/works_r/ch09/ch15/datas/twitter.csv",
header=T,
stringsAsFactors = F,
fileEncoding = "UTF-8")
twitter<- read.csv("C:/works/works_r/ch09/ch15/datas/twitter.csv",
header=T,
stringsAsFactors = F,
fileEncoding = "UTF-8")
twitter<-read.csv("C:/works/works_r/ch09/ch15/datas/twitter.csv",
header=T,
stringsAsFactors = F,
fileEncoding = "UTF-8")
songlist <-read_excel("C:/works/works_r/ch09/ch15/SongList.xlsx")
install.packages("readxl")
library(readxl)
library(rJava)
library(tm)
library(multilinguer)
library(KoNLP)
library(dplyr)
twitter<-read.csv("C:/works/works_r/ch09/ch15/datas/twitter.csv",
header=T,
stringsAsFactors = F,
fileEncoding = "UTF-8")
twitter<-read.csv("C:/works/works_r/ch09/ch15/datas/twitter.csv",
header=T,
stringsAsFactors = F,
fileEncoding = "UTF-8")
